# 9장. 웹 크롤러 설계

- 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 목적 
- **크롤러 이용 방식**
  - ```검색 엔진 인덱싱``` : 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만듦<br/>(ex.) 구글 검색 엔진
  - ```웹 아카이빙``` : 나중에 사용할 목적으로 장기보관하기 위에 웹에서 정보를 모으는 절차
  - ```웹 마이닝``` : 인터넷에서 유용한 지식 도출해 냄
  - ```웹 모니터링``` : 인터넷에서 저작권/상표권이 침해되는 사례 모니터링

---
### 1단계. 문제 이해 및 설계 범위 확장
> **웹 크롤러의 기본 알고리즘**
> 1. URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 모든 웹 페이지를 다운로드
> 2. 다운받은 웹 페이지에서 URL들을 추출
> 3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복
<br/><br/>
- **웹 크롤러가 만족해야 하는 속성**
  - ```규모 확장성``` : 병행성을 활용해서 효과적인 웹 크롤링 가능
  - ```안정성``` : 비정상적인 입력이나 환경에 잘 대응할 수 있어야 함
  - ```예절``` : 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안됨
  - ```확장성``` : 새로운 형태의 콘텐츠를 지원하기 쉬워야 함 
  
- 개략적 규모 추정
  - 매달 10억 개의 웹 페이지를 다운로드
  - QPS=10억/30일/24시간/3600초=대략 400페이지/초
    - QPS(Query Per Second)=요청이 초당으로 얼마나 처리되는지. 시스템의 성능과 처리 능력을 평가하는 중요한 측정 지표
  - peak QPS=2XQPS=800
  - 웹 페이지의 크기 평균은 500k라 가정
  - 10억 페이지x500k=500TB/월
  - 30TB의 저장용량 필요
  
<br/><Br/>

### 2단계. 개략적 설계안 제시 및 동의 구하기

![alt text](/minseo/images/image.png)

- **시작 URL 집합**
  - 웹 크롤러가 크롤링을 시작하는 출발점
  - 크롤러가 가능한 한 많은 링크를 탐색할 수 있도록 하는 URL을 고르는 것이 바람직함 
  - 일반적으로 전체 URL 공간을 작은 부분집합으로 나누는 전략 
  - 주제별로 다른 시작 URL을 사용하는 전략

- **미수집 URL 저장소**
  - 크롤링 상태 : (1) 다운로드 할 URL (2) 다운로드 된 URL 
  - (1)의 경우, 미수집 URL 저장소에서 관리. 
  - ```FIFO Queue```

- **HTML 다운로드**
  - 인터넷에서 웹 페이지를 다운로드하는 컴포넌트

- **도메인 이름 변환기**
  - URL에 대응되는 IP주소를 알아냄 
  
- **콘텐츠 파서**
  - ```파싱```/```검증``` 절차
  - 크롤링 서버 안에 콘텐츠 파서를 구현하면 크롤링 과정이 느려져 독립된 컴포넌트를 만듦

- **중복 콘텐츠인가?**
  - 중복된 콘텐츠인지 비교/확인하는 방법 -> 웹 페이지의 ```해시 값``` 비교

- **콘텐츠 저장소**
  - HTML 문서 보관
  - 저장할 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려
  
  ✔️ 디스크와 메모리를 동시에 사용하는 저장소로 선택!
    - 데이터의 양이 너무 많음 -> 대부분의 콘텐츠는 디스크에 저장
    - 인기있는 콘텐츠 -> 메모리에
  
- **URL 추출기**
  - HTML 페이지를 파싱하여 링크들을 골라내는 역할 

- **URL 필터**
  - 특정한 콘텐츠 타입, 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL -> 크롤링 대상에서 배제

- **이미 방문한 URL?**
  - 구현방법) 이미 방문한 URLorURL 저장소에 보관된 URL을 추적할 수 있도록 하는 자료구조 사용
  - 이미 방문한 URL 추적 -> 서버 부하를 줄이고 시스템이 무한 루프에 빠지는 일을 방지할 수 있음 
  - 자료구조) ```블룸 필터```, ```해시 테이블```
  
- **URL 저장소**
  - 이미 방문한 URL 보관
<br/><br/>

**✅ 웹 크롤러 작업 흐름**
![alt text](/minseo/images/image-4.png)
1. 시작 URL들을 ```미수집 URL 저장소```에 저장
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져옴
3. HTML 다운로더는 도메인 이름 변환기 -> URL의 IP 주소를 알아내고 해당 IP주소로 접속하여 웹 페이지를 다운 받음 
4. ```콘텐츠 파서```는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증
5. 콘텐츠 파싱 & 검증 끝나면 **중복 컨텐츠**인지 확인하는 절차 개시 
6. 중복 콘텐츠인지 확인하기 위해 해당 페이지가 이미 저장소에 있는지 본다
    - 이미 저장소에 O -> 처리하지 않고 버림
    - 저장소에 X -> 저장소에 저장한 뒤 URL 추출기로 전달
7. URL 추출기는 해당 HTML 페이지에서 링크를 골라낸다
8. 골라낸 링크를 URL 필터로 전달
9. 필터링이 끝나고 남은 URL만 **중복 URL** 판별 단계로 
10. 이미 처리한 URL인지 확인하기 위해 URL 저장소에 보관된 URL인지 살핌. 이미 저장소에 있는 URL은 버림 
11. 저장소에 없는 URL은 URL 저장소에 저장할 뿐 아니라 미수집 URL 에도 전달.  

---

### 3단계. 상세 설계
> **DFS를 쓸 것 인가, BFS를 쓸 것인가**
- 웹은 유향 그래프 : node=페이지, edge=URL
- DFS : 그래프 크기가 클 경우 어느정도로 깊숙이 가게 될지 가늠하기 어려움
- BFS : FIFO 큐 사용. 한 쪽으로는 탐색할 URL을 집어넣고 다른 한쪽으로는 꺼내기만 함 
  - **문제점 1)** **한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아감.** <br/>크롤러는 같은 호스트에 속한 많은 링크 다운 -> 바빠짐 -> 링크들을 병렬로 처리 시, 위키피디아 서버는 수많은 요청으로 과부하에 걸리게 됨 
  - **문제점 2) 우선순위가 없음**
    <br/>모든 웹 페이지가 같은 수준의 품질, 중요성을 갖지 않으므로 우선순위가 존재해야 함


> **미수집 URL 저장소**
= 다운로드할 URL 보관하는 장소

1. **```예의바른 크롤러```**
- 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청한다 
- 같은 웹 사이트의 페이지를 다운받는 태스크 -> 시간차를 두고 실행하도록 해야 함 <br/>
➡️ ```웹사이트의 호스트명``` & ```다운로드를 수행하는 작업스레드``` 사이의 관계를 유지해야<br/><br/>
∴ **각 다운로드 스레드는 별도 FIFO 큐를 가짐 -> 해당 큐에서 꺼낸 URL만 다운로드함** 
<br/><br/>
![alt text](image-1.png)

- ```큐 라우터``` = 같은 호스트에 속한 URL은 언제나 같은 큐 (b1,b2,..,bn)에 가도록 보장<br/>
![alt text](image.png)
<br/>
- ```매핑 테이블``` = 호스트 이름과 큐 사이의 관계를 보관하는 테이블
- ```FIFO 큐(b1~bn)``` = 같은 호스트에 속한 URL은 언제나 같은 큐에 보관됨 
- ```queue selector``` = 큐들을 순회하면서 큐에서 URL을 꺼내서 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에게 전달하는 역할
- ```worker thread``` = 작업 스레드는 **전달된 URL을 다운로드**하는 작업을 수행. 전달된 URL은 **순차적**으로 처리될 것이며 **작업들 사이에는 일정한 delay**를 둘 수 있음 

<br/><br/><br/>
2. **```우선순위```**
- 유용성에 따라 URL의 우선 순위를 나눌 때는 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도를 사용할 수 있음 <br/>

✅ "**순위 결정 장치**" = URL 우선순위를 정하는 컴포넌트
![alt text](image-2.png)
<br/>
- ```prioritizer``` = URL을 입력 받아 우선순위 계산
- ```큐(f1,...,fn)``` = 우선순위 별로 큐가 하나씩 할당됨. 우선순위가 높으면 선택될 확률도 올라감
- ```큐 선택기``` = 임의 큐에서 처리할 URL을 꺼내는 역할. 순위가 높은 큐에서 더 자주 꺼내도록 프로그램 되어 있음 
<br/>
![alt text](image-3.png)
<br/>
- ```front queue``` = 우선 순위 결정 과정을 처리
- ```back queue``` = 크롤러가 예의 바르게 동작하도록 보증

<br/><br/><br/>

3. **```신선도```**
- 이미 다운로드한 페이지여도 주기적으로 recrawl할 필요 O 
- 해당 작업을 최적화 하기 위해서는
  - **웹 페이지의 변경 이력(update history) 활용**
  - **우선 순위를 활용하여 중요한 페이지는 좀 더 자주 재수집** 

- 미수집 URL 저장소를 위한 지속성 저장장치 
  - 검색 엔진을 위한 크롤러 
  - URL 전부를 메모리에 보관 -> 안정성/규모 확장성 측면에서 바람직X
  - 전부 디스크에 저장 -> 느려서 쉽게 성능 병목지점이 됨 <br/>
  ✅ **대부분의 URL은 디스크에, IO 비용을 줄이기 위해 메모리 버퍼에 큐를 둔다. 버퍼에 있는 데이터는 주기적으로 디스크에 기록** 
  <br/>
  ❓왜 하필 queue에 저장하는 걸까? 
  ❗
  <br/><br/>


> HTML 다운로더
- HTTP 프로토콜을 통해 웹 페이즈를 내려받음
- ```Robots.txt```
  - 웹 사이트가 크롤라와 소통하는 표준적 방법
  - 크롤러가 수집해도 되는 페이지 목록이 들어 있음. 크롤러는 해당 파일에 나열된 규칙을 먼저 확인해야 함 

<br/>
- ```성능 최적화```